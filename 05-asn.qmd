---
title: "Written Assignment 05"
author: "Simone Mayers"
date: today
date-format: long
number-sections: true
number-depth: 3
format:
  html:
    toc: true
    toc-location: right
    number-sections: true
    number-depth: 3
    html-math-method: katex
    embed-resources: true
    self-contained: true
    #bibliography: dasc-6000.bib 
    #csl: ieee-with-url.csl
    linkcolor: red
    urlcolor: blue 
    link-citations: yes
    #header-includes:
    #- \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Assignment Goal {.unnumbered}

The goal of this assignment is to develop $k$-nearest neighbor algorithms for machine learning applications. 

# Assessment Rubric {.unnumbered}

There are five questions and each question carries twenty points. Please show all your work. If you just provide the final answer, no credit will be given. If you make any assumptions, please state them clearly.
    
# A Nearest Neighbor Machine Learning Algorithm

The table below lists a dataset that was used to create a nearest neighbor model that predicts whether it will be a good day to go surfing.

```{r}
#| echo: false
ID <- c(1,2,3,4,5,6)
wave_size <- c(6,1,7,7,2,10)
wave_period <- c(15,6,10,12,2,2)
wind_speed <- c(5,9,4,3,10,20)
good_surf <- c("yes", "no", "yes", "yes", "no", "no")
df1 <- data.frame(ID, wave_size, wave_period, wind_speed, good_surf)
print(df1)
```
```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df1, caption = 'Data for a predicting a good day for surfing.', col.names = c('ID', 'Wave Size (ft)', 'Wave Period (secs)', 'Wind Speed (MPH/hr)', 'Good to Surf'), align = "lcccl")
```

```{r}
# Function to calculate Euclidean distance between two points
euclidean_distance <- function(x1, x2) {
  sqrt(sum((x1 - x2) ^ 2))
}

# K-nearest neighbor function using df1
knn_predict <- function(query_point, df1, k = 1) {
  # Extract the features from df1
  features <- df1[, c("wave_size", "wave_period", "wind_speed")]
  target <- df1$good_surf
  
  # Calculate distances between query point and all points in df1
  distances <- apply(features, 1, function(row) euclidean_distance(query_point, row))
  
  # Find the indices of the k nearest neighbors
  nearest_neighbors <- order(distances)[1:k]
  
  # Get the labels of the nearest neighbors
  nearest_labels <- target[nearest_neighbors]
  
  # Return the most common label (majority vote)
  prediction <- names(sort(table(nearest_labels), decreasing = TRUE))[1]
  
  return(prediction)
}

# Define query points
query_points <- data.frame(
  wave_size = c(8, 8, 6),
  wave_period = c(15, 2, 11),
  wind_speed = c(2, 18, 4)
)

# Predict the label for each query point using df1
for (i in 1:nrow(query_points)) {
  query <- query_points[i, ]
  prediction <- knn_predict(query, df1, k = 1)  # Using k = 1 (Nearest Neighbor)
  
  cat("Query", i, ": Wave Size =", query$wave_size, "ft, Wave Period =", query$wave_period, "secs, Wind Speed =", query$wind_speed, "MPH/hr \n")
  cat("Prediction (Good to Surf):", prediction, "\n\n")
}

```
I am using Euclidean distance to measure how close a query point is to the points in the dataset.
All features (wave_size, wave_period, wind_speed) are treated equally.
The prediction is based on the most common label among the 𝑘-nearest neighbors. Since 𝑘=1 here, the single closest neighbor determines the prediction.



# Eamil Spam Filtering
 
Email spam filtering models often use a bag-of-words representation for emails. In a bag-of-words representation, the descriptive features that describe a document (in our case, an email) each represent how many times a particular word occurs in the document. One descriptive feature is included for each word in a predefined dictionary. The dictionary is typically defined as the complete set of words that occur in the training dataset.

The table below lists the bag-of-words representation for the following five emails and a target feature, spam, whether they are spam emails or genuine emails:

1. money, money, money
1. free money of free gambling fun
1. gambling of fun
1. machine learning of fun, fun, fun
1. free machine learning


```{r}
#| echo: false
ID <- c(1,2,3,4,5)
money <- c(3,1,0,0,0)
free <- c(0,2,0,0,1)
of <- c(0,1,1,1,0)
gambling <- c(0,1,1,0,0)
fun <- c(0,1,1,3,0)
machine <- c(0,0,0,1,1)
learning <- c(0,0,0,1,1)
spam <- c("true", "true", "true", "false", "false")
df3 <- data.frame(ID, money, free, of, gambling, fun, machine, learning, spam)
print(df3)
```

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df3, caption = 'Email spam filtering dataset.', col.names = c('ID', 'money', 'free', 'of', 'gambling', 'fun', 'machine', 'learning', 'spam'), align = "lcccccccr")
```

1. What target level would a nearest neighbor model using **Euclidean distance** return for the following email: machine learning of free?

```{r}
# Define the query email's bag-of-words representation
query_email <- data.frame(
  money = 0,
  free = 1,
  of = 1,
  gambling = 0,
  fun = 0,
  machine = 1,
  learning = 1
)

# Extract the feature columns from the dataset
features <- df3[, c("money", "free", "of", "gambling", "fun", "machine", "learning")]
target <- df3$spam  # Spam column is the target

# Calculate the Euclidean distance between the query email and each email in the dataset
distances <- apply(features, 1, function(row) euclidean_distance(query_email, row))

# Find the nearest neighbor (k = 1)
nearest_neighbor <- which.min(distances)

# Output the nearest neighbor and the predicted label
print(paste("The nearest neighbor is email ID:", df3$ID[nearest_neighbor]))
print(paste("Prediction (Spam or Not):", target[nearest_neighbor]))

```

I assume that Euclidean distance is appropriate for this text classification problem. While Euclidean distance works, other distance metrics are often used in text-based tasks.
I assume that the bag-of-words representation sufficiently captures the content of each email. This model does not take into account word order or the meaning of words.
All words are treated equally in the calculation of distances. This may not always be ideal; some words could be more important than others.


2. What target level would a $k\!-\!NN$ model with $k = 3$ and using the **Euclidean distance** return for the same query?

```{r}
# Define the query email's bag-of-words representation
query_email <- data.frame(
  money = 0,
  free = 1,
  of = 1,
  gambling = 0,
  fun = 0,
  machine = 1,
  learning = 1
)

# Function to calculate Euclidean distance between two points
euclidean_distance <- function(x1, x2) {
  sqrt(sum((x1 - x2) ^ 2))
}

# Extract the feature columns from the dataset
features <- df3[, c("money", "free", "of", "gambling", "fun", "machine", "learning")]
target <- df3$spam  # Spam column is the target

# Calculate the Euclidean distance between the query email and each email in the dataset
distances <- apply(features, 1, function(row) euclidean_distance(query_email, row))

# Sort distances and get the indices of the k = 3 nearest neighbors
k <- 3
nearest_neighbors <- order(distances)[1:k]

# Get the labels of the nearest neighbors
nearest_labels <- target[nearest_neighbors]

# Output the nearest neighbors' IDs and their labels
print(paste("The nearest neighbors are email IDs:", df3$ID[nearest_neighbors]))
print(paste("The spam status of the 3 nearest neighbors:", nearest_labels))

# Perform majority voting to get the prediction
prediction <- names(sort(table(nearest_labels), decreasing = TRUE))[1]
print(paste("Prediction (Spam or Not):", prediction))

```

3. What target level would a weighted $k\!-\!NN$ model with $k = 5$ and using a **weighting scheme** of the **reciprocal of the squared Euclidean distance** between the neighbor and the query, return for the query?

```{r}
# Calculate the Euclidean distance between the query email and each email in the dataset
distances <- apply(features, 1, function(row) euclidean_distance(query_email, row))

# Sort distances and get the indices of the k = 5 nearest neighbors
k <- 5
nearest_neighbors <- order(distances)[1:k]

# Get the labels and distances of the nearest neighbors
nearest_labels <- target[nearest_neighbors]
nearest_distances <- distances[nearest_neighbors]

# Calculate the weights (reciprocal of squared distances)
weights <- 1 / (nearest_distances ^ 2)

# Output the nearest neighbors' IDs, their labels, and their weights
print(paste("The nearest neighbors are email IDs:", df3$ID[nearest_neighbors]))
print(paste("The spam status of the 5 nearest neighbors:", nearest_labels))
print(paste("Weights of the 5 nearest neighbors:", weights))

# Perform weighted voting
weighted_votes <- tapply(weights, nearest_labels, sum)

# Return the label with the highest weighted vote
prediction <- names(which.max(weighted_votes))
print(paste("Prediction (Spam or Not):", prediction))

```


4. What target level would a $k\!-\!NN$ model with $k = 3$ and using the **Manhattan distance** return for the same query?

```{r}
# Manhattan distance function
manhattan_distance <- function(x1, x2) {
  sum(abs(x1 - x2))
}

# Apply k-NN with Manhattan distance
manhattan_distances <- apply(features, 1, function(row) manhattan_distance(query_email, row))
k <- 3
nearest_neighbors_manhattan <- order(manhattan_distances)[1:k]
nearest_labels_manhattan <- target[nearest_neighbors_manhattan]

# Output results
print(paste("The nearest neighbors are email IDs:", df3$ID[nearest_neighbors_manhattan]))
print(paste("The spam status of the 3 nearest neighbors:", nearest_labels_manhattan))

# Majority voting
prediction_manhattan <- names(sort(table(nearest_labels_manhattan), decreasing = TRUE))[1]
print(paste("Prediction (Spam or Not):", prediction_manhattan))

```
I only swapped out the distance calculation with Manhattan distance and kept the rest of the $k$-NN logic intact. The prediction is based on majority voting among the 3 nearest neighbors.


5. There are a lot of zero entries in the spam bag-of-words dataset. This is indicative of sparse data and is typical for text analytics. Cosine similarity is often a good choice when dealing with sparse non-binary data. What target level would a $3-NN$ model using the cosine similarity return for the query?

```{r}
# Cosine similarity function
cosine_similarity <- function(x1, x2) {
  dot_product <- sum(x1 * x2)
  magnitude_x1 <- sqrt(sum(x1^2))
  magnitude_x2 <- sqrt(sum(x2^2))
  return(dot_product / (magnitude_x1 * magnitude_x2))
}

# Apply k-NN with cosine similarity
cosine_similarities <- apply(features, 1, function(row) cosine_similarity(query_email, row))
k <- 3
nearest_neighbors_cosine <- order(-cosine_similarities)[1:k]  # Sort by descending cosine similarity
nearest_labels_cosine <- target[nearest_neighbors_cosine]

# Output results
print(paste("The nearest neighbors are email IDs:", df3$ID[nearest_neighbors_cosine]))
print(paste("The spam status of the 3 most similar neighbors:", nearest_labels_cosine))

# Majority voting
prediction_cosine <- names(sort(table(nearest_labels_cosine), decreasing = TRUE))[1]
print(paste("Prediction (Spam or Not):", prediction_cosine))

```


# Corruption Prediction

The predictive task in this question is to predict the level of corruption in a country based on a range of macro-economic and social features. The following table lists some countries described by the following descriptive features:

1. **Life Exp** -- the mean life expectancy at birth

2. **Top-10 Income** -- the percentage of the annual income of the country that goes to the top 10\% of earners

3. **Infant Mor** -- the number of infant deaths per 1,000 births

4. **Mil Spend** -- the percentage of GDP spent on the military

5. **School Years** -- the mean number years spent in school by adult females

The target feature is the **Corruption Perception Index** (CPI). The **CPI** measures the perceived levels of corruption in the public sector of countries and ranges from 0 (highly corrupt) to 100 (very clean).


```{r}
#| echo: false
library(readr)
df4 <- readr::read_csv("./corruption-data.csv", col_names = TRUE)
column_names <- c("Country ID", "Life Expectancy", "Top-10 Income", "Infant Mortality", "Military Spending", "School Years", "CPI")
colnames(df4) <- column_names
summary(df4)
print(df4)
```

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df4, caption = 'Corruption data of various countries.', align = "lrrrrrr")
```


I will use Russia as our query country for CPI prediction with the following values for the descriptive features:

- Life Expectancy = 67.62
- Top-10 Income = 31.68
- Infant Mortality = 10.0
- Military Spending = 3.87
- School Years = 12.9

a.  What value would a 3-nearest neighbor prediction model using Euclidean distance return for the CPI of Russia?

```{r}
# Query point for Russia
query_russia <- data.frame(
  Life_Expectancy = 67.62,
  Top_10_Income = 31.68,
  Infant_Mortality = 10.0,
  Military_Spending = 3.87,
  School_Years = 12.9
)

# Extract features and CPI
features <- df4[, c("Life Expectancy", "Top-10 Income", "Infant Mortality", "Military Spending", "School Years")]
target_cpi <- df4$CPI

# Calculate Euclidean distances between Russia and all countries
distances_russia <- apply(features, 1, function(row) euclidean_distance(query_russia, row))

# Find the 3 nearest neighbors
k <- 3
nearest_neighbors_russia <- order(distances_russia)[1:k]
nearest_cpi_russia <- target_cpi[nearest_neighbors_russia]

# Output the CPI values and the predicted CPI
print(paste("The CPI values of the 3 nearest neighbors are:", nearest_cpi_russia))
predicted_cpi_russia <- mean(nearest_cpi_russia)
print(paste("The predicted CPI for Russia is:", predicted_cpi_russia))

```
The model calculates the Euclidean distance between Russia and all countries in the dataset. It then selects the 3 nearest neighbors and averages their CPI values to produce the predicted CPI for Russia.



b. What value would a weighted $k\!-\!NN$ prediction model return for the CPI of Russia? Use $k = 16$ (i.e., the full dataset) and a weighting scheme of the reciprocal of the squared Euclidean distance between the neighbor and the query.

```{r}
# Avoid division by zero by adding a small value to the distances
distances_russia <- apply(features, 1, function(row) euclidean_distance(query_russia, row))
distances_russia <- ifelse(distances_russia == 0, 1e-6, distances_russia)

# Calculate the weights (reciprocal of squared distances)
weights_russia <- 1 / (distances_russia ^ 2)

# Calculate the weighted CPI prediction
weighted_cpi_russia <- sum(weights_russia * target_cpi) / sum(weights_russia)

# Output the final predicted CPI for Russia
print(paste("The weighted predicted CPI for Russia is:", weighted_cpi_russia))

```

The Euclidean distance between Russia and each country in the dataset is calculated based on the macro-economic and social features.
The weight of each country is computed as the reciprocal of the squared distance between the country and Russia. Countries closer to Russia (in terms of features) will have a larger weight, meaning their CPI values will have a bigger influence on the final prediction.
The prediction is the weighted average of the CPI values, with the weights determined by the distances.


c. The descriptive features in this dataset are of different types. For example, some are percentages, others are measured in years, and others are measured in counts per 1,000. We should always consider normalizing our data, but it is particularly important to do this when the descriptive features are measured in different units. What value would a 3-nearest neighbor prediction model using Euclidean distance return for the CPI of Russia when the descriptive features have been normalized using range normalization?

```{r}
# Ensure all features are numeric
features <- data.frame(lapply(features, as.numeric))

# Function for range normalization
range_normalize <- function(column) {
  (column - min(column)) / (max(column) - min(column))
}

# Normalize features and the query
features_normalized <- as.data.frame(lapply(features, range_normalize))
query_russia_normalized <- as.data.frame(lapply(1:ncol(query_russia), function(i) {
  range_normalize(c(features[, i], query_russia[, i]))[nrow(features) + 1]
}))
names(query_russia_normalized) <- names(features)
query_russia_normalized <- as.data.frame(t(query_russia_normalized))

# Calculate Euclidean distances using normalized features
distances_russia_normalized <- apply(features_normalized, 1, function(row) euclidean_distance(query_russia_normalized, row))

# Find the 3 nearest neighbors and predict CPI
nearest_neighbors_normalized <- order(distances_russia_normalized)[1:k]
nearest_cpi_normalized <- target_cpi[nearest_neighbors_normalized]
predicted_cpi_russia_normalized <- mean(nearest_cpi_normalized)

# Output results
print(paste("The CPI values of the 3 nearest neighbors are:", nearest_cpi_normalized))
print(paste("The predicted CPI for Russia after normalization is:", predicted_cpi_russia_normalized))

```

This method scales each feature to a [0, 1] range, which ensures that the features measured in different units do not disproportionately affect the Euclidean distance calculation.
After normalizing, I apply the $k$-Nearest Neighbor algorithm as before, using Euclidean distance to find the 3 closest countries to Russia in terms of normalized features.
The predicted CPI for Russia is the average CPI of the 3 nearest neighbors based on the normalized data.


d.  What value would a weighted $k\!-\!NN$ prediction model—with $k = 16$ (i.e., the full dataset) and using a weighting scheme of the reciprocal of the squared Euclidean distance between the neighbor and the query—return for the CPI of Russia when it is applied to the range-normalized data?

```{r}
# Normalize features and the query
features_normalized <- as.data.frame(lapply(features, range_normalize))
query_russia_normalized <- as.data.frame(lapply(1:ncol(query_russia), function(i) {
  range_normalize(c(features[, i], query_russia[, i]))[nrow(features) + 1]
}))
names(query_russia_normalized) <- names(features)
query_russia_normalized <- as.data.frame(t(query_russia_normalized))

# Calculate Euclidean distances using normalized features
distances_russia_normalized <- apply(features_normalized, 1, function(row) euclidean_distance(query_russia_normalized, row))
distances_russia_normalized <- ifelse(distances_russia_normalized == 0, 1e-6, distances_russia_normalized)

# Calculate the weights (reciprocal of squared distances)
weights_russia_normalized <- 1 / (distances_russia_normalized ^ 2)

# Calculate the weighted CPI prediction
weighted_cpi_russia_normalized <- sum(weights_russia_normalized * target_cpi) / sum(weights_russia_normalized)

# Output the final predicted CPI for Russia after range normalization
print(paste("The weighted predicted CPI for Russia after normalization is:", weighted_cpi_russia_normalized))

```

All features were normalized using range normalization, ensuring that each feature contributes equally to the distance calculation, despite being measured in different units.
The model uses weighted voting, where the weight of each neighbor is the reciprocal of its squared Euclidean distance from the query. Closer neighbors have more influence on the predicted CPI.
I use 𝑘= 16 (i.e., all countries in the dataset) for the prediction, with each country contributing to the final CPI value according to its weight.


e.  The actual 2011 CPI for Russia was 2.4488. Which of the predictions made was the most accurate? Why do you think this was?

The prediction with the smallest absolute error will be the most accurate. The reason a particular method is more accurate could be due to:

Normalizing the data can lead to better predictions when the features are on different scales. Normalization helps prevent features with larger ranges from dominating the distance calculations.

Weighted 𝑘-NN places more emphasis on closer neighbors, which can provide more accurate predictions, especially when there is a non-linear relationship between the features and the target variable.

The most accurate prediction is the one from [part X], because its absolute error is the smallest. This indicates that this model was better able to capture the relationship between the descriptive features and the CPI.

If the most accurate prediction comes from one of the normalized models, this suggests that normalizing the features helped balance the different scales of the data. If it comes from a weighted model, this suggests that giving more weight to closer neighbors improved the prediction accuracy.



# Recommender systems

You have been given the job of building a recommender system for a large online shop that has a stock of over 100,000 items. In this domain the behavior of customers is captured in terms of what items they have bought or not bought. For example, the following table lists the behavior of two customers in this domain for a subset of the items that at least one of the customers has bought.

```{r}
#| echo: false
c1 <- c(1, 2)
c2 <- c("true", "true")
c3 <- c("true", "false")
c4 <- c("true", "false")
c5 <- c("false", "true")
c6 <- c("false", "true")
df5 <- data.frame(c1, c2, c3, c4, c5, c6)
column_names <- c("ID", "Item 107", "Item 498", "Item 7256", "Item 28063", "Item 75328")
colnames(df5) <- column_names
print(df5)
```


```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df5, caption = 'Recommender system training dataset.', align = "lrrrrr")
```


```{r}
#| echo: false
c1 <- c("Query")
c2 <- c("true")
c3 <- c("false")
c4 <- c("true")
c5 <- c("false")
c6 <- c("false")
df6 <- data.frame(c1, c2, c3, c4, c5, c6)
column_names <- c("ID", "Item 107", "Item 498", "Item 7256", "Item 28063", "Item 75328")
colnames(df6) <- column_names
print(df6)
```


The company has decided to use a similarity-based model to implement the recommender system. 

1. Which of the following three similarity indexes do you think the system should be based on?

    $$
    \begin{align*}
    \text{Russell-Rao}(X,Y) &= \frac{CP(X,Y)}{P} \\ 
    \text{Sokal-Michener}(X,Y) &= \frac{CP(X,Y)+CA(X,Y)}{P} \\ 
    \text{Jaccard}(X,Y) &= \frac{CP(X,Y)}{CP(X,Y)+PA(X,Y)+AP(X,Y)}
    \end{align*}
    $$
    
Assumptions:
The dataset is binary, where TRUE represents a purchased item and FALSE represents a non-purchased item.
I assume that most items have not been purchased by either customer, which is common in large datasets like online retail.
I am aiming to find the similarity between customers based on their purchase behaviors to recommend items.

```{r}
# Function to calculate CP, CA, PA, and AP for two customers
calculate_similarity_values <- function(cust1, cust2) {
  CP <- sum(cust1 == TRUE & cust2 == TRUE)
  CA <- sum(cust1 == FALSE & cust2 == FALSE)
  PA <- sum(cust1 == TRUE & cust2 == FALSE)
  AP <- sum(cust1 == FALSE & cust2 == TRUE)
  
  return(list(CP = CP, CA = CA, PA = PA, AP = AP))
}

# Apply the function to calculate similarity values between the query and each customer
query_customer <- df6[1, 2:6]  # Exclude the ID column
customer_1 <- df5[1, 2:6]  # First customer
customer_2 <- df5[2, 2:6]  # Second customer

# Calculate for customer 1
similarity_cust1 <- calculate_similarity_values(query_customer, customer_1)

# Calculate for customer 2
similarity_cust2 <- calculate_similarity_values(query_customer, customer_2)

# Output the results
print("Similarity values with customer 1:")
print(similarity_cust1)

print("Similarity values with customer 2:")
print(similarity_cust2)

```

```{r}

# Function to calculate the similarity indexes
calculate_indexes <- function(similarity_values, P) {
  CP <- similarity_values$CP
  CA <- similarity_values$CA
  PA <- similarity_values$PA
  AP <- similarity_values$AP
  
  # Russell-Rao
  russell_rao <- CP / P
  
  # Sokal-Michener
  sokal_michener <- (CP + CA) / P
  
  # Jaccard
  denominator_jaccard <- CP + PA + AP
  if (denominator_jaccard == 0) {
    jaccard <- 0  # Handle division by zero
  } else {
    jaccard <- CP / denominator_jaccard
  }
  
  return(list(Russell_Rao = russell_rao, Sokal_Michener = sokal_michener, Jaccard = jaccard))
}

# P represents the total number of items, which is 5 in this case.
P <- ncol(df6) - 1  # Total number of items, excluding the ID column

# Calculate similarity indexes for customer 1
indexes_cust1 <- calculate_indexes(similarity_cust1, P)

# Calculate similarity indexes for customer 2
indexes_cust2 <- calculate_indexes(similarity_cust2, P)

# Output similarity indexes
print("Similarity indexes with customer 1:")
print(indexes_cust1)

print("Similarity indexes with customer 2:")
print(indexes_cust2)


```

The Russell-Rao only considers mutual purchases, and in a sparse dataset, may not perform well since it ignores non-purchases.
The Sokal-Michener considers both mutual purchases and non-purchases, but this could inflate similarity in a sparse dataset.
The Jaccar focuses on mutual purchases and excludes shared non-purchases, making it more appropriate for sparse datasets.
Since customer purchase data is likely sparse, Jaccard is the most appropriate index to use, as it emphasizes common purchases and excludes irrelevant non-purchases.





2. What items will the system recommend to the following customer? Assume that the recommender system uses the similarity index you chose in the first part of this question and is trained on the sample dataset listed above. Also assume that the system generates recommendations for query customers by finding the customer most similar to them in the dataset and then recommending the items that this similar customer has bought but that the query customer has not bought.

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df6, caption = 'Recommender system query instance.', align = "lrrrrr")
```

```{r}


```

```{r}
# Function to calculate Jaccard Index
calculate_jaccard <- function(similarity_values) {
  CP <- similarity_values$CP
  PA <- similarity_values$PA
  AP <- similarity_values$AP
  
  denominator_jaccard <- CP + PA + AP
  if (denominator_jaccard == 0) {
    return(0)  # Handle division by zero
  } else {
    return(CP / denominator_jaccard)
  }
}

# Calculate Jaccard similarity for both customers
jaccard_cust1 <- calculate_jaccard(similarity_cust1)
jaccard_cust2 <- calculate_jaccard(similarity_cust2)

# Output the Jaccard similarity values
print(paste("Jaccard similarity with customer 1:", jaccard_cust1))
print(paste("Jaccard similarity with customer 2:", jaccard_cust2))

# Find the most similar customer (choose the one with higher Jaccard index)
if (jaccard_cust1 > jaccard_cust2) {
  most_similar_customer <- 1
} else if (jaccard_cust2 > jaccard_cust1) {
  most_similar_customer <- 2
} else {
  # If both are equal, select randomly
  most_similar_customer <- sample(c(1, 2), 1)
}

print(paste("The most similar customer is customer:", most_similar_customer))

# Step 3: Recommend items
# Get the purchase data for the most similar customer and the query customer
similar_customer_purchases <- df5[most_similar_customer, 2:6]  # Exclude the ID column
query_customer_purchases <- df6[1, 2:6]  # Exclude the ID column

# Recommend items that the similar customer has bought but the query customer has not
recommended_items <- names(which(similar_customer_purchases == TRUE & query_customer_purchases == FALSE))

# Output the recommended items
if (length(recommended_items) > 0) {
  print(paste("Items to recommend to the query customer:", paste(recommended_items, collapse = ", ")))
} else {
  print("No items to recommend to the query customer.")
}


```



# Rent Prediction

You have been asked by a San Francisco property investment company to create a predictive model that will generate house price estimates for properties they are considering purchasing as rental properties. The table below lists a sample of properties that have recently been sold for rental in the city. The descriptive features in this dataset are **Size** (the property size in square feet) and **Rent** (the estimated monthly rental value of the property in dollars). The target feature, **Price**, lists the prices that these properties were sold for in dollars.

```{r}
#| echo: false
c1 <- c(1, 2, 3, 4, 5, 6, 7)
c2 <- c(2700, 1315, 1050, 2200, 1800, 1900, 960)
c3 <- c(9235, 1800, 1250, 7000, 3800, 4000, 800)
c4 <- c(2000000, 820000, 800000, 1750000, 1450000, 1500500, 720000)
df7 <- data.frame(c1, c2, c3, c4)
column_names <- c("ID", "Size", "Rent", "Price")
colnames(df7) <- column_names
print(df7)
```

```{r}
#| echo: false
#| tbl-cap-location: margin
knitr::kable(df7, caption = 'Rental property dataset.', align = "lrrr")
```

1. Create a $k-d$ tree for this dataset. Assume the following order over the features: Rent, then Size.

```{r}
# Recursive function to build the k-d tree with proper base case handling
build_kd_tree <- function(data, depth = 0) {
  # Base case: If no data, return NULL
  if (nrow(data) == 0) {
    return(NULL)
  }
  
  # Base case: If only one row, return the row as a leaf node
  if (nrow(data) == 1) {
    return(list(data = data, left = NULL, right = NULL))
  }
  
  # Alternate between Rent (depth % 2 == 0) and Size (depth % 2 == 1)
  feature <- ifelse(depth %% 2 == 0, "Rent", "Size")
  
  # Sort the data by the selected feature
  data <- data[order(data[[feature]]), ]
  
  # Find the median index
  median_index <- floor(nrow(data) / 2) + 1
  
  # Split the data into left and right subtrees
  left <- data[1:(median_index - 1), ]
  right <- data[(median_index + 1):nrow(data), ]
  
  # Recursively build the tree
  node <- list(
    data = data[median_index, ],
    left = build_kd_tree(left, depth + 1),
    right = build_kd_tree(right, depth + 1)
  )
  
  return(node)
}

# Build the k-d tree using your dataset (df7)
kd_tree <- build_kd_tree(df7)

# Function to print the k-d tree with proper recursion limits
print_kd_tree <- function(node, depth = 0) {
  if (is.null(node)) {
    return()
  }
  
  # Indentation based on depth
  indent <- paste(rep("  ", depth), collapse = "")
  
  # Print the current node's data
  print(paste(indent, "Node:", toString(node$data)))
  
  # Recursively print left and right children, if they exist
  if (!is.null(node$left)) {
    print_kd_tree(node$left, depth + 1)
  }
  
  if (!is.null(node$right)) {
    print_kd_tree(node$right, depth + 1)
  }
}

# Print the constructed k-d tree
print_kd_tree(kd_tree)



```



2. Using the $k\!-\!d$ tree that you created in the first part of this question, find the nearest neighbor to the following query: Size = 1,000, Rent = 2,200.

```{r}
# Function to calculate Euclidean distance between two points
euclidean_distance <- function(query, point) {
  return(sqrt((query["Size"] - point["Size"])^2 + (query["Rent"] - point["Rent"])^2))
}

# Nearest neighbor search function
find_nearest_neighbor <- function(node, query, depth = 0, best = NULL) {
  if (is.null(node)) {
    return(best)  # Return the best so far if the node is null
  }
  
  # Calculate distance between the query and the current node
  current_distance <- euclidean_distance(query, node$data)
  
  # Update best if the current node is closer
  if (is.null(best) || current_distance < euclidean_distance(query, best$data)) {
    best <- node
  }
  
  # Determine whether to go left or right in the tree
  feature <- ifelse(depth %% 2 == 0, "Rent", "Size")  # Alternate between Rent and Size
  
  # Traverse the tree
  if (query[[feature]] < node$data[[feature]]) {
    best <- find_nearest_neighbor(node$left, query, depth + 1, best)
  } else {
    best <- find_nearest_neighbor(node$right, query, depth + 1, best)
  }
  
  # Check the other side of the tree if necessary
  difference <- abs(query[[feature]] - node$data[[feature]])
  if (difference < euclidean_distance(query, best$data)) {
    if (query[[feature]] < node$data[[feature]]) {
      best <- find_nearest_neighbor(node$right, query, depth + 1, best)
    } else {
      best <- find_nearest_neighbor(node$left, query, depth + 1, best)
    }
  }
  
  return(best)
}

# Query to find nearest neighbor for
query <- data.frame(Size = 1000, Rent = 2200)

# Find nearest neighbor using the kd_tree
nearest_neighbor <- find_nearest_neighbor(kd_tree, query[1, ])

# Print the nearest neighbor found
print("Nearest Neighbor:")
print(nearest_neighbor$data)


```




